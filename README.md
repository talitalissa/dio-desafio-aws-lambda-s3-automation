# Desafio de Projeto DIO: Executando Tarefas Automatizadas com Lambda Function e S3

Este repositÃ³rio documenta a soluÃ§Ã£o do Desafio de Projeto da [Digital Innovation One (DIO)](https://www.dio.me/) sobre a automaÃ§Ã£o de tarefas usando AWS Lambda e Amazon S3.

O objetivo foi aplicar os conceitos de **arquitetura orientada a eventos** e **infraestrutura como cÃ³digo (IaC)**. Utilizamos um template **AWS CloudFormation** para provisionar e "conectar" todos os recursos necessÃ¡rios para que um upload de arquivo em um bucket S3 dispare automaticamente uma funÃ§Ã£o Lambda para processamento.

---

## ðŸŽ¯ Objetivo

O desafio consistiu em implementar um pipeline de automaÃ§Ã£o serverless. O foco Ã© documentar o processo, a arquitetura e os principais conceitos de permissÃ£o e triggers que fazem essa integraÃ§Ã£o funcionar.

---

## ðŸ“– Conceitos-Chave da Arquitetura

Esta arquitetura Ã© um dos padrÃµes mais comuns e poderosos da nuvem AWS.

* **Amazon S3 (Simple Storage Service):** Ã‰ o nosso serviÃ§o de armazenamento de objetos. Neste projeto, ele atua como o **"gatilho" (trigger)**. NÃ³s o usamos para duas finalidades:
    1.  `Bucket de Input`: Recebe os arquivos originais. O upload de um novo arquivo *inicia* o workflow.
    2.  `Bucket de Output`: Armazena o resultado do processamento da Lambda.
* **AWS Lambda:** Ã‰ o nosso serviÃ§o de computaÃ§Ã£o "serverless" (sem servidor). Ele fornece o cÃ³digo que executa em resposta ao evento do S3. NÃ£o precisamos gerenciar servidores; a funÃ§Ã£o apenas "acorda", executa e "dorme".
* **AWS CloudFormation (IaC):** Em vez de criar os buckets, a funÃ§Ã£o e as permissÃµes manualmente pelo console (o que Ã© sujeito a erros), nÃ³s definimos todos eles em um Ãºnico arquivo **YAML**. O CloudFormation lÃª esse "mapa" e constrÃ³i toda a infraestrutura e suas conexÃµes de forma automatizada e 100% repetÃ­vel.
* **IAM (PermissÃµes):** A parte mais crÃ­tica da automaÃ§Ã£o. Para que isso funcione, duas permissÃµes sÃ£o necessÃ¡rias:
    1.  A *Lambda* precisa de permissÃ£o para ler do bucket de input e escrever no bucket de output (via **IAM Role**).
    2.  O *S3* precisa de permissÃ£o para invocar a funÃ§Ã£o Lambda (via **Lambda Permission**).

---

## âš™ï¸ O Projeto: Pipeline de Processamento de Arquivos

O workflow implementado por este template Ã© o seguinte:

1.  Um usuÃ¡rio (ou sistema) faz o upload de um arquivo (ex: `teste.txt`) no `BucketDeInput`.
2.  O S3 detecta o evento `s3:ObjectCreated:*` (criaÃ§Ã£o de novo objeto).
3.  O S3, que tem permissÃ£o, invoca a `MinhaFuncaoLambda`, enviando os detalhes do evento (qual arquivo, em qual bucket).
4.  A `MinhaFuncaoLambda` (escrita em Python) Ã© executada. Ela:
    * LÃª o `teste.txt` do `BucketDeInput`.
    * Processa o conteÃºdo (neste exemplo, converte todo o texto para MAIÃšSCULAS).
    * Salva um novo arquivo (ex: `resultado-teste.txt`) no `BucketDeOutput`.
5.  O fluxo termina, e a Lambda Ã© finalizada.

### Diagrama Conceitual do Fluxo

```mermaid
graph TD;
    style Input fill:#D82233,color:#fff
    style Output fill:#D82233,color:#fff
    style Lambda fill:#FF9900,color:#fff
    
    Usuario[UsuÃ¡rio] -->|1. Upload 'teste.txt'| Input(S3 - Bucket de Input);
    Input -->|2. Evento s3:ObjectCreated| Lambda(AWS Lambda - ProcessarTexto);
    Lambda -->|3. GetObject 'teste.txt'| Input;
    Lambda -->|4. PutObject 'RESULTADO.txt'| Output(S3 - Bucket de Output);
